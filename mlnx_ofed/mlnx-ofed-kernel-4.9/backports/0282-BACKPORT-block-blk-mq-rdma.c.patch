From: Israel Rukshin <israelr@mellanox.com>
Subject: [PATCH] BACKPORT: block/blk-mq-rdma.c

Change-Id: Ic385dadc1f1a2869d8721cb87ac5ee7eb47dc1c7
---
 block/blk-mq-rdma.c | 26 ++++++++++++++++++++++++++
 1 file changed, 26 insertions(+)

--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -1,3 +1,4 @@
+#ifdef HAVE_BLK_MQ_TAG_SET_HAS_MAP
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (c) 2017 Sagi Grimberg.
@@ -21,6 +22,7 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+#ifdef HAVE_BLK_MQ_RDMA_MAP_QUEUES_MAP
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
@@ -41,4 +43,28 @@ int blk_mq_rdma_map_queues(struct blk_mq
 fallback:
 	return blk_mq_map_queues(map);
 }
+#else
+int blk_mq_rdma_map_queues(struct blk_mq_tag_set *set,
+		struct ib_device *dev, int first_vec)
+{
+	const struct cpumask *mask;
+	unsigned int queue, cpu;
+
+	for (queue = 0; queue < set->nr_hw_queues; queue++) {
+		mask = ib_get_vector_affinity(dev, first_vec + queue);
+		if (!mask)
+			goto fallback;
+
+		for_each_cpu(cpu, mask)
+			set->map[0].mq_map[cpu] = queue;
+	}
+
+	return 0;
+
+fallback:
+	return blk_mq_map_queues(&set->map[0]);
+}
+#endif
 EXPORT_SYMBOL_GPL(blk_mq_rdma_map_queues);
+
+#endif /* HAVE_BLK_MQ_TAG_SET_HAS_MAP */
