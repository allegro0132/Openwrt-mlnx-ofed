From: Talat Batheesh <talatb@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_tc.c

Change-Id: I9f86790002164aea7b666ef5a4d23630a8db74c3
---
 .../net/ethernet/mellanox/mlx5/core/en_tc.c   | 779 +++++++++++++++---
 1 file changed, 679 insertions(+), 100 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@ -11,14 +11,14 @@
  *     without modification, are permitted provided that the following
  *     conditions are met:
  *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
+ *	- Redistributions of source code must retain the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer.
  *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
+ *	- Redistributions in binary form must reproduce the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer in the documentation and/or other materials
+ *	  provided with the distribution.
  *
  * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
  * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
@@ -30,37 +30,137 @@
  * SOFTWARE.
  */
 
-#include <net/flow_dissector.h>
 #include <net/sch_generic.h>
 #include <net/pkt_cls.h>
+#ifdef HAVE_TC_GACT_H
 #include <net/tc_act/tc_gact.h>
+#endif
+#ifdef HAVE_IS_TCF_SKBEDIT_MARK
 #include <net/tc_act/tc_skbedit.h>
+#endif
 #include <linux/mlx5/fs.h>
 #include <linux/mlx5/device.h>
+#include <lib/devcom.h>
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #include <linux/rhashtable.h>
 #include <linux/refcount.h>
-#include <linux/completion.h>
+#endif
+#include <net/switchdev.h>
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #include <net/tc_act/tc_mirred.h>
+#endif
+#ifdef HAVE_IS_TCF_VLAN
 #include <net/tc_act/tc_vlan.h>
+#endif
+#ifdef HAVE_TCF_TUNNEL_INFO
 #include <net/tc_act/tc_tunnel_key.h>
+#endif
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
+#include <linux/tc_act/tc_pedit.h>
 #include <net/tc_act/tc_pedit.h>
+#endif
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
+#include <net/tc_act/tc_csum.h>
+#endif
+#include <linux/completion.h>
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 #include <net/tc_act/tc_csum.h>
+#endif
 #ifdef HAVE_MINIFLOW
 #include <net/tc_act/tc_ct.h>
 #endif
 #include <net/arp.h>
+#ifdef HAVE_TC_FLOWER_OFFLOAD
+#include <net/flow_dissector.h>
+#endif
+#ifdef HAVE_IPV6_STUBS_H
 #include <net/ipv6_stubs.h>
+#endif
 #include <net/bonding.h>
 #include "en.h"
 #include "en_rep.h"
 #include "en_tc.h"
 #include "eswitch.h"
-#include "miniflow.h"
+#ifdef HAVE_TC_FLOWER_OFFLOAD
+ #include "miniflow.h"
+#endif
 #include "fs_core.h"
 #include "en/port.h"
 #include "en/tc_tun.h"
 #include "lib/devcom.h"
 #include "lib/geneve.h"
+#include <linux/mlx5/vport.h>
+#include <net/flow_offload.h>
+
+#include <net/tc_act/tc_pedit.h>
+
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && \
+    defined(HAVE_IS_TCF_GACT_GOTO_CHAIN) && \
+    defined(HAVE_FLOWER_MULTI_MASK)
+#define PRIO_CHAIN_SUPPORT 1
+#endif
+
+#if defined(HAVE_TC_FLOWER_OFFLOAD) && \
+    !defined(HAVE_SWITCHDEV_PORT_SAME_PARENT_ID)
+#include <net/bonding.h>
+
+bool switchdev_port_same_parent_id(struct net_device *a,
+				   struct net_device *b)
+{
+	struct mlx5e_priv *priv_a, *priv_b;
+	struct mlx5_eswitch *peer_esw;
+	struct mlx5_devcom *devcom;
+	struct net_device *ndev;
+	struct bonding *bond;
+	bool ret = true;
+
+	if (netif_is_bond_master(b)) {
+		bond = netdev_priv(b);
+		if (!bond_has_slaves(bond))
+			return false;
+
+		rcu_read_lock();
+#ifdef for_each_netdev_in_bond_rcu
+		for_each_netdev_in_bond_rcu(b, ndev) {
+#else
+		for_each_netdev_in_bond(b, ndev) {
+#endif
+			ret &= switchdev_port_same_parent_id(a, ndev);
+			if (!ret)
+				break;
+		}
+		rcu_read_unlock();
+		return ret;
+	}
+
+	if (!(a->features & NETIF_F_HW_TC) || !(b->features & NETIF_F_HW_TC))
+		return false;
+
+	priv_a = netdev_priv(a);
+	priv_b = netdev_priv(b);
+
+	if (!priv_a->mdev->priv.eswitch || !priv_b->mdev->priv.eswitch)
+		return false;
+
+	if (priv_a->mdev->priv.eswitch->mode != MLX5_ESWITCH_OFFLOADS ||
+	    priv_b->mdev->priv.eswitch->mode != MLX5_ESWITCH_OFFLOADS)
+		return false;
+
+	if (priv_a->mdev == priv_b->mdev)
+		return true;
+
+	devcom = priv_a->mdev->priv.devcom;
+	peer_esw = mlx5_devcom_get_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+	if (!peer_esw)
+		return false;
+
+	ret = (peer_esw->dev == priv_b->mdev);
+	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+	return ret;
+}
+#endif
+
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 
 #define MLX5E_TC_TABLE_NUM_GROUPS 4
 #define MLX5E_TC_TABLE_MAX_GROUP_SIZE BIT(16)
@@ -99,28 +199,6 @@ struct mlx5e_hairpin_entry {
 	struct completion hw_res_created;
 };
 
-struct mod_hdr_key {
-	int num_actions;
-	void *actions;
-};
-
-struct mlx5e_mod_hdr_entry {
-	/* a node of a hash table which keeps all the mod_hdr entries */
-	struct hlist_node mod_hdr_hlist;
-
-	/* protects flows list */
-	spinlock_t flows_lock;
-	/* flows sharing the same mod_hdr entry */
-	struct list_head flows;
-
-	struct mod_hdr_key key;
-
-	struct mlx5_modify_hdr *modify_hdr;
-
-	refcount_t refcnt;
-	struct completion hw_res_created;
-};
-
 static void mlx5e_tc_del_flow(struct mlx5e_priv *priv,
 			      struct mlx5e_tc_flow *flow);
 
@@ -209,6 +287,31 @@ static bool mlx5e_is_simple_flow(struct
 	return flow_flag_test(flow, SIMPLE);
 }
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
+struct mod_hdr_key {
+	int num_actions;
+	void *actions;
+};
+
+struct mlx5e_mod_hdr_entry {
+	/* a node of a hash table which keeps all the mod_hdr entries */
+	struct hlist_node mod_hdr_hlist;
+
+	/* protects flows list */
+	spinlock_t flows_lock;
+	/* flows sharing the same mod_hdr entry */
+	struct list_head flows;
+
+	struct mod_hdr_key key;
+
+	struct mlx5_modify_hdr *modify_hdr;
+
+	refcount_t		refcnt;
+	struct completion hw_res_created;
+
+	//struct rcu_head rcu;
+};
+
 static inline u32 hash_mod_hdr_info(struct mod_hdr_key *key)
 {
 	return jhash(key->actions,
@@ -371,6 +474,7 @@ static void mlx5e_detach_mod_hdr(struct
 	mlx5e_mod_hdr_put(priv, flow->mh, namespace);
 	flow->mh = NULL;
 }
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX */
 
 static
 struct mlx5_core_dev *mlx5e_hairpin_get_mdev(struct net *net, int ifindex)
@@ -663,7 +767,6 @@ static void mlx5e_hairpin_put(struct mlx
 }
 
 #define UNKNOWN_MATCH_PRIO 8
-
 static int mlx5e_hairpin_get_prio(struct mlx5e_priv *priv,
 				  struct mlx5_flow_spec *spec, u8 *match_prio,
 				  struct netlink_ext_ack *extack)
@@ -672,6 +775,7 @@ static int mlx5e_hairpin_get_prio(struct
 	u8 prio_val, prio_mask = 0;
 	bool vlan_present;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	if (priv->dcbx_dp.trust_state != MLX5_QPTS_TRUST_PCP) {
 		NL_SET_ERR_MSG_MOD(extack,
@@ -679,6 +783,7 @@ static int mlx5e_hairpin_get_prio(struct
 		return -EOPNOTSUPP;
 	}
 #endif
+#endif
 	headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, outer_headers);
 	headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value, outer_headers);
 
@@ -723,8 +828,7 @@ static int mlx5e_hairpin_flow_add(struct
 	}
 
 	peer_id = MLX5_CAP_GEN(peer_mdev, vhca_id);
-	err = mlx5e_hairpin_get_prio(priv, &parse_attr->spec, &match_prio,
-				     extack);
+	err = mlx5e_hairpin_get_prio(priv, &parse_attr->spec, &match_prio, extack);
 	if (err)
 		return err;
 
@@ -881,6 +985,7 @@ mlx5e_tc_add_nic_flow(struct mlx5e_priv
 		attr->counter = counter;
 	}
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR) {
 		err = mlx5e_attach_mod_hdr(priv, flow, parse_attr);
 		flow_act.modify_hdr = attr->modify_hdr;
@@ -888,6 +993,7 @@ mlx5e_tc_add_nic_flow(struct mlx5e_priv
 		if (err)
 			return err;
 	}
+#endif
 
 	mutex_lock(&priv->fs.tc.t_lock);
 	if (IS_ERR_OR_NULL(priv->fs.tc.t)) {
@@ -910,8 +1016,10 @@ mlx5e_tc_add_nic_flow(struct mlx5e_priv
 							    MLX5E_TC_FT_LEVEL, 0);
 		if (IS_ERR(priv->fs.tc.t)) {
 			mutex_unlock(&priv->fs.tc.t_lock);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Failed to create tc offload table\n");
+#endif
 			netdev_err(priv->netdev,
 				   "Failed to create tc offload table\n");
 			return PTR_ERR(priv->fs.tc.t);
@@ -949,13 +1057,16 @@ static void mlx5e_tc_del_nic_flow(struct
 	}
 	mutex_unlock(&priv->fs.tc.t_lock);
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
 		mlx5e_detach_mod_hdr(priv, flow);
+#endif
 
 	if (flow_flag_test(flow, HAIRPIN))
 		mlx5e_hairpin_flow_del(priv, flow);
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void mlx5e_detach_encap(struct mlx5e_priv *priv,
 			       struct mlx5e_tc_flow *flow, int out_index);
 
@@ -989,6 +1100,7 @@ mlx5e_tc_offload_fdb_rules(struct mlx5_e
 
 	return rule;
 }
+#endif
 
 static void
 mlx5e_tc_unoffload_fdb_rules(struct mlx5_eswitch *esw,
@@ -1003,6 +1115,7 @@ mlx5e_tc_unoffload_fdb_rules(struct mlx5
 	mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static struct mlx5_flow_handle *
 mlx5e_tc_offload_to_slow_path(struct mlx5_eswitch *esw,
 			      struct mlx5e_tc_flow *flow,
@@ -1022,6 +1135,7 @@ mlx5e_tc_offload_to_slow_path(struct mlx
 
 	return rule;
 }
+#endif
 
 static void
 mlx5e_tc_unoffload_from_slow_path(struct mlx5_eswitch *esw,
@@ -1095,11 +1209,13 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 	struct mlx5e_tc_flow_parse_attr *parse_attr = attr->parse_attr;
 	u16 max_prio = mlx5_eswitch_get_prio_range(esw);
-	struct net_device *out_dev, *encap_dev = NULL;
 	struct mlx5_fc *counter = NULL;
+#ifdef HAVE_TCF_TUNNEL_INFO
+	struct net_device *out_dev, *encap_dev = NULL;
 	struct mlx5e_rep_priv *rpriv;
 	struct mlx5e_priv *out_priv;
 	bool encap_valid = true;
+#endif
 	int err = 0;
 	int out_index;
 
@@ -1123,6 +1239,7 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		return -EOPNOTSUPP;
 	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++) {
 		int mirred_ifindex;
 
@@ -1133,7 +1250,8 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		out_dev = __dev_get_by_index(dev_net(priv->netdev),
 					     mirred_ifindex);
 		err = mlx5e_attach_encap(priv, flow, out_dev, out_index,
-					 extack, &encap_dev, &encap_valid);
+					 extack,
+					 &encap_dev, &encap_valid);
 		if (err)
 			return err;
 
@@ -1142,11 +1260,13 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		attr->dests[out_index].rep = rpriv->rep;
 		attr->dests[out_index].mdev = out_priv->mdev;
 	}
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 	err = mlx5_eswitch_add_vlan_action(esw, attr);
 	if (err)
 		return err;
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR) {
 		err = mlx5e_attach_mod_hdr(priv, flow, parse_attr);
 		kfree(parse_attr->mod_hdr_actions);
@@ -1154,6 +1274,7 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		if (err)
 			return err;
 	}
+#endif
 
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_COUNT) {
 		counter = mlx5_fc_create(attr->counter_dev, true);
@@ -1163,6 +1284,7 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		attr->counter = counter;
 	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	/* we get here if one of the following takes place:
 	 * (1) there's no error
 	 * (2) there's an encap action and we don't have valid neigh
@@ -1180,6 +1302,7 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		return PTR_ERR(flow->rule[0]);
 	else
 		flow_flag_set(flow, OFFLOADED);
+#endif
 
 	return 0;
 }
@@ -1223,16 +1346,19 @@ static void mlx5e_tc_del_fdb_flow_simple
 
 	mlx5_eswitch_del_vlan_action(esw, attr);
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++)
 		if (attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP)
 			mlx5e_detach_encap(priv, flow, out_index);
+#endif
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
 		mlx5e_detach_mod_hdr(priv, flow);
+#endif
 
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_COUNT)
 		mlx5_fc_destroy(attr->counter_dev, attr->counter);
-
 }
 
 static void mlx5e_tc_del_fdb_flow(struct mlx5e_priv *priv,
@@ -1249,12 +1375,25 @@ static void mlx5e_tc_del_fdb_flow(struct
 			mlx5_fc_destroy(priv->mdev, flow->dummy_counter);
 	}
 
+#if defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX) && defined(HAVE_TCF_TUNNEL_INFO)
 	if (attr->parse_attr) {
 		kfree(attr->parse_attr->mod_hdr_actions);
 		kvfree(attr->parse_attr);
 	}
+#endif
 }
 
+#if defined(HAVE_TCF_TUNNEL_INFO) || defined(HAVE_TC_CLSFLOWER_STATS)
+static struct mlx5_fc *mlx5e_tc_get_counter(struct mlx5e_tc_flow *flow)
+{
+	if (mlx5e_is_eswitch_flow(flow))
+		return flow->esw_attr->counter;
+	else
+		return flow->nic_attr->counter;
+}
+#endif
+
+#ifdef HAVE_TCF_TUNNEL_INFO
 void mlx5e_tc_encap_flows_add(struct mlx5e_priv *priv,
 			      struct mlx5e_encap_entry *e,
 			      struct list_head *flow_list)
@@ -1371,14 +1510,6 @@ void mlx5e_tc_encap_flows_del(struct mlx
 	}
 }
 
-static struct mlx5_fc *mlx5e_tc_get_counter(struct mlx5e_tc_flow *flow)
-{
-	if (mlx5e_is_eswitch_flow(flow))
-		return flow->esw_attr->counter;
-	else
-		return flow->nic_attr->counter;
-}
-
 /* Takes reference to all flows attached to encap and adds the flows to
  * flow_list using 'tmp_list' list_head in mlx5e_tc_flow.
  */
@@ -1451,27 +1582,34 @@ retry:
 
 	return next;
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 void mlx5e_tc_update_neigh_used_value(struct mlx5e_neigh_hash_entry *nhe)
 {
 	struct mlx5e_neigh *m_neigh = &nhe->m_neigh;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct mlx5e_encap_entry *e = NULL;
 	u64 bytes, packets, lastuse = 0;
 	struct mlx5e_tc_flow *flow;
 	struct mlx5_fc *counter;
+#endif
 	struct neigh_table *tbl;
 	bool neigh_used = false;
 	struct neighbour *n;
 
-	if (m_neigh->family == AF_INET)
+	if (m_neigh->family == AF_INET) {
 		tbl = &arp_tbl;
-#if IS_ENABLED(CONFIG_IPV6)
-	else if (m_neigh->family == AF_INET6)
-		tbl = &nd_tbl;
+#if defined(HAVE_IPV6_STUBS_H) && IS_ENABLED(CONFIG_IPV6)
+	} else if (m_neigh->family == AF_INET6) {
+		if (!ipv6_stub || !ipv6_stub->nd_tbl)
+			return;
+		tbl = ipv6_stub->nd_tbl;	
 #endif
-	else
+	} else {
 		return;
+	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	/* mlx5e_get_next_valid_encap() releases previous encap before returning
 	 * next one.
 	 */
@@ -1501,13 +1639,16 @@ void mlx5e_tc_update_neigh_used_value(st
 		}
 		mutex_unlock(&esw->offloads.encap_tbl_lock);
 
+
 		mlx5e_put_encap_flow_list(priv, &flow_list);
 		if (neigh_used) {
 			/* release current encap before breaking the loop */
 			mlx5e_encap_put(priv, e);
 			break;
 		}
+
 	}
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 	if (neigh_used) {
 		nhe->reported_lastuse = jiffies;
@@ -1524,6 +1665,7 @@ void mlx5e_tc_update_neigh_used_value(st
 	}
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void mlx5e_encap_dealloc(struct mlx5e_priv *priv, struct mlx5e_encap_entry *e)
 {
 	struct encap_id_entry *ei, *tmp;
@@ -1580,6 +1722,7 @@ static void mlx5e_detach_encap(struct ml
 
 	mlx5e_encap_dealloc(priv, e);
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static void __mlx5e_tc_del_fdb_peer_flow(struct mlx5e_tc_flow *flow)
 {
@@ -1630,25 +1773,41 @@ static void mlx5e_tc_del_flow(struct mlx
 	}
 }
 
-
+#ifdef HAVE_TCF_TUNNEL_INFO
 static int parse_tunnel_attr(struct mlx5e_priv *priv,
 			     struct mlx5_flow_spec *spec,
 			     struct tc_cls_flower_offload *f,
-			     struct net_device *filter_dev, u8 *match_level)
+			     struct net_device *filter_dev, u8 *match_level
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+				 , struct flow_rule *rule
+#endif
+				)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				       outer_headers);
 	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
 				       outer_headers);
+#ifdef HAVE_TC_SETUP_FLOW_ACTION
 	struct flow_rule *rule = tc_cls_flower_offload_flow_rule(f);
+#endif
 	struct flow_match_control enc_control;
 	int err;
 
 	err = mlx5e_tc_tun_parse(filter_dev, priv, spec, f,
-				 headers_c, headers_v, match_level);
+				 headers_c, headers_v, match_level
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+				 , rule
+#endif
+				       );
 	if (err) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
+#else
+		netdev_err(priv->netdev,
+#endif
 				   "failed to parse tunnel attributes");
 		return err;
 	}
@@ -1697,6 +1856,7 @@ static int parse_tunnel_attr(struct mlx5
 		MLX5_SET(fte_match_set_lyr_2_4, headers_v, ethertype, ETH_P_IPV6);
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_IP
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_IP)) {
 		struct flow_match_ip match;
 
@@ -1720,12 +1880,17 @@ static int parse_tunnel_attr(struct mlx5
 		    !MLX5_CAP_ESW_FLOWTABLE_FDB
 			(priv->mdev,
 			 ft_field_support.outer_ipv4_ttl)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
+#else
+			netdev_err(priv->netdev,
+#endif
 					   "Matching on TTL is not supported");
 			return -EOPNOTSUPP;
 		}
 
 	}
+#endif
 
 	/* Enforce DMAC when offloading incoming tunneled flows.
 	 * Flow counters require a match on the DMAC.
@@ -1741,6 +1906,7 @@ static int parse_tunnel_attr(struct mlx5
 
 	return 0;
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static void *get_match_headers_criteria(u32 flags,
 					struct mlx5_flow_spec *spec)
@@ -1767,18 +1933,30 @@ static int __parse_cls_flower(struct mlx
 			      struct tc_cls_flower_offload *f,
 			      struct net_device *filter_dev,
 			      u8 *inner_match_level, u8 *outer_match_level,
-			      bool *is_tunnel_flow)
+			      bool *is_tunnel_flow
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+			      , struct flow_rule *rule
+#endif
+			      )
+
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				       outer_headers);
 	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
 				       outer_headers);
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN
 	void *misc_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				    misc_parameters);
 	void *misc_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
 				    misc_parameters);
+#endif
+
+#ifdef HAVE_TC_SETUP_FLOW_ACTION
 	struct flow_rule *rule = tc_cls_flower_offload_flow_rule(f);
+#endif
 	struct flow_dissector *dissector = rule->match.dissector;
 	u16 addr_type = 0;
 	u8 ip_proto = 0;
@@ -1790,30 +1968,57 @@ static int __parse_cls_flower(struct mlx
 	    ~(BIT(FLOW_DISSECTOR_KEY_CONTROL) |
 	      BIT(FLOW_DISSECTOR_KEY_BASIC) |
 	      BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN
 	      BIT(FLOW_DISSECTOR_KEY_VLAN) |
+#else
+	      BIT(FLOW_DISSECTOR_KEY_VLANID) |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN
 	      BIT(FLOW_DISSECTOR_KEY_CVLAN) |
+#endif
 	      BIT(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
+#ifdef HAVE_TCF_TUNNEL_INFO
 	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_KEYID) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_PORTS)	|
 	      BIT(FLOW_DISSECTOR_KEY_ENC_CONTROL) |
+#else
+	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_TCP
 	      BIT(FLOW_DISSECTOR_KEY_TCP) |
 	      BIT(FLOW_DISSECTOR_KEY_IP)  |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_IP
+	      BIT(FLOW_DISSECTOR_KEY_IP)  |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_IP
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IP) |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_OPTS
 	      BIT(FLOW_DISSECTOR_KEY_ENC_OPTS))) {
+#else
+		0)) {
+#endif
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "Unsupported key");
+#endif
 		netdev_warn(priv->netdev, "Unsupported key used: 0x%x\n",
 			    dissector->used_keys);
 		return -EOPNOTSUPP;
 	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	if ((flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) ||
 	     flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_KEYID) ||
-	     flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_PORTS) ||
-	     flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_OPTS)) &&
+	     flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_PORTS)
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_OPTS
+	     || flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_OPTS)
+#endif
+	     ) &&
 	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_CONTROL)) {
 		struct flow_match_control match;
 
@@ -1821,7 +2026,12 @@ static int __parse_cls_flower(struct mlx
 		switch (match.key->addr_type) {
 		case FLOW_DISSECTOR_KEY_IPV4_ADDRS:
 		case FLOW_DISSECTOR_KEY_IPV6_ADDRS:
-			if (parse_tunnel_attr(priv, spec, f, filter_dev, outer_match_level))
+			if (parse_tunnel_attr(priv, spec, f, filter_dev, outer_match_level
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+					      , rule
+#endif
+
+						))
 				return -EOPNOTSUPP;
 			break;
 		default:
@@ -1838,6 +2048,7 @@ static int __parse_cls_flower(struct mlx
 						    spec);
 		*is_tunnel_flow = true;
 	}
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_BASIC)) {
 		struct flow_match_basic match;
@@ -1851,6 +2062,7 @@ static int __parse_cls_flower(struct mlx
 		if (match.mask->n_proto)
 			*match_level = MLX5_MATCH_L2;
 	}
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_VLAN) ||
 	    is_vlan_dev(filter_dev)) {
 		struct flow_dissector_key_vlan filter_dev_mask;
@@ -1860,7 +2072,9 @@ static int __parse_cls_flower(struct mlx
 		if (is_vlan_dev(filter_dev)) {
 			match.key = &filter_dev_key;
 			match.key->vlan_id = vlan_dev_vlan_id(filter_dev);
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN_TPID
 			match.key->vlan_tpid = vlan_dev_vlan_proto(filter_dev);
+#endif
 			match.key->vlan_priority = 0;
 			match.mask = &filter_dev_mask;
 			memset(match.mask, 0xff, sizeof(*match.mask));
@@ -1868,6 +2082,7 @@ static int __parse_cls_flower(struct mlx
 		} else {
 			flow_rule_match_vlan(rule, &match);
 		}
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN
 		if (match.mask->vlan_id ||
 		    match.mask->vlan_priority ||
 		    match.mask->vlan_tpid) {
@@ -1882,6 +2097,11 @@ static int __parse_cls_flower(struct mlx
 				MLX5_SET(fte_match_set_lyr_2_4, headers_v,
 					 cvlan_tag, 1);
 			}
+#else
+		if (match.mask->vlan_id || match.mask->vlan_priority) {
+			MLX5_SET(fte_match_set_lyr_2_4, headers_c, cvlan_tag, 1);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_v, cvlan_tag, 1);
+#endif /* HAVE_FLOW_DISSECTOR_KEY_CVLAN */
 
 			MLX5_SET(fte_match_set_lyr_2_4, headers_c, first_vid,
 				 match.mask->vlan_id);
@@ -1895,6 +2115,25 @@ static int __parse_cls_flower(struct mlx
 
 			*match_level = MLX5_MATCH_L2;
 		}
+#else /* HAVE_FLOW_DISSECTOR_KEY_VLAN */
+	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_VLANID)) {
+		struct flow_dissector_key_tags *key =
+			skb_flow_dissector_target(f->dissector,
+						  FLOW_DISSECTOR_KEY_VLANID,
+						  f->key);
+		struct flow_dissector_key_tags *mask =
+			skb_flow_dissector_target(f->dissector,
+						  FLOW_DISSECTOR_KEY_VLANID,
+						  f->mask);
+		if (mask->vlan_id) {
+			MLX5_SET(fte_match_set_lyr_2_4, headers_c, cvlan_tag, 1);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_v, cvlan_tag, 1);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_c, first_vid, mask->vlan_id);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_v, first_vid, key->vlan_id);
+
+			*match_level = MLX5_MATCH_L2;
+		}
+#endif /* HAVE_FLOW_DISSECTOR_KEY_VLAN */
 	} else if (*match_level != MLX5_MATCH_NONE) {
 		/* cvlan_tag enabled in match criteria and
 		 * disabled in match value means both S & C tags
@@ -1904,6 +2143,7 @@ static int __parse_cls_flower(struct mlx
 		*match_level = MLX5_MATCH_L2;
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN 
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_CVLAN)) {
 		struct flow_match_vlan match;
 
@@ -1935,6 +2175,7 @@ static int __parse_cls_flower(struct mlx
 			*match_level = MLX5_MATCH_L2;
 		}
 	}
+#endif /* HAVE_FLOW_DISSECTOR_KEY_CVLAN */
 
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {
 		struct flow_match_eth_addrs match;
@@ -2042,6 +2283,7 @@ static int __parse_cls_flower(struct mlx
 			*match_level = MLX5_MATCH_L3;
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_IP
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_IP)) {
 		struct flow_match_ip match;
 
@@ -2064,7 +2306,11 @@ static int __parse_cls_flower(struct mlx
 		if (match.mask->ttl &&
 		    !MLX5_CAP_ESW_FLOWTABLE_FDB(priv->mdev,
 						ft_field_support.outer_ipv4_ttl)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
+#else
+			netdev_err(priv->netdev,
+#endif
 					   "Matching on TTL is not supported");
 			return -EOPNOTSUPP;
 		}
@@ -2072,6 +2318,7 @@ static int __parse_cls_flower(struct mlx
 		if (match.mask->tos || match.mask->ttl)
 			*match_level = MLX5_MATCH_L3;
 	}
+#endif
 
 	/* ***  L3 attributes parsing up to here *** */
 
@@ -2104,8 +2351,10 @@ static int __parse_cls_flower(struct mlx
 				 udp_dport, ntohs(match.key->dst));
 			break;
 		default:
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Only UDP and TCP transports are supported for L4 matching");
+#endif
 			netdev_err(priv->netdev,
 				   "Only UDP and TCP transport are supported\n");
 			return -EINVAL;
@@ -2115,6 +2364,7 @@ static int __parse_cls_flower(struct mlx
 			*match_level = MLX5_MATCH_L4;
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_TCP
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_TCP)) {
 		struct flow_match_tcp match;
 
@@ -2127,7 +2377,7 @@ static int __parse_cls_flower(struct mlx
 		if (match.mask->flags)
 			*match_level = MLX5_MATCH_L4;
 	}
-
+#endif
 	return 0;
 }
 
@@ -2178,9 +2428,15 @@ static int parse_cls_flower(struct mlx5e
 			    struct mlx5e_tc_flow *flow,
 			    struct mlx5_flow_spec *spec,
 			    struct tc_cls_flower_offload *f,
-			    struct net_device *filter_dev)
+			    struct net_device *filter_dev
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+			    , struct flow_rule *rule
+#endif
+				)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5_core_dev *dev = priv->mdev;
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
@@ -2201,7 +2457,12 @@ static int parse_cls_flower(struct mlx5e
 
 	err = __parse_cls_flower(priv, spec, f, filter_dev,
 				 &inner_match_level, &outer_match_level,
-				 &is_tunnel_flow);
+				 &is_tunnel_flow
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+				 , rule
+#endif
+				);
+
 	non_tunnel_match_level = (inner_match_level == MLX5_MATCH_NONE) ?
 				 outer_match_level : inner_match_level;
 
@@ -2211,8 +2472,10 @@ static int parse_cls_flower(struct mlx5e
 		if (rep->vport != MLX5_VPORT_UPLINK &&
 		    (esw->offloads.inline_mode != MLX5_INLINE_MODE_NONE &&
 		    esw->offloads.inline_mode < non_tunnel_match_level)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Flow is not offloaded due to min inline setting");
+#endif
 			netdev_warn(priv->netdev,
 				    "Flow is not offloaded due to min inline setting, required %d actual %d\n",
 				    non_tunnel_match_level, esw->offloads.inline_mode);
@@ -2230,7 +2493,7 @@ static int parse_cls_flower(struct mlx5e
 
 	return err;
 }
-
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 static int pedit_header_offsets[] = {
 	[FLOW_ACT_MANGLE_HDR_TYPE_ETH] = offsetof(struct pedit_headers, eth),
 	[FLOW_ACT_MANGLE_HDR_TYPE_IP4] = offsetof(struct pedit_headers, ip4),
@@ -2399,15 +2662,19 @@ static int offload_pedit_fields(struct p
 			continue;
 
 		if (s_mask && a_mask) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "can't set and add to the same HW field");
+#endif
 			printk(KERN_WARNING "mlx5: can't set and add to the same HW field (%x)\n", f->field);
 			return -EOPNOTSUPP;
 		}
 
 		if (nactions == max_actions) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "too many pedit actions, can't offload");
+#endif
 			printk(KERN_WARNING "mlx5: parsed %d pedit actions, can't do more\n", nactions);
 			return -EOPNOTSUPP;
 		}
@@ -2452,8 +2719,10 @@ static int offload_pedit_fields(struct p
 		next_z = find_next_zero_bit(&mask, f->field_bsize, first);
 		last  = find_last_bit(&mask, f->field_bsize);
 		if (first < next_z && next_z < last) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "rewrite of few sub-fields isn't supported");
+#endif
 			printk(KERN_WARNING "mlx5: rewrite of few sub-fields (mask %lx) isn't offloaded\n",
 			       mask);
 			return -EOPNOTSUPP;
@@ -2539,7 +2808,7 @@ int parse_tc_pedit_action(struct mlx5e_p
 	err = -EOPNOTSUPP; /* can't be all optimistic */
 
 	if (htype == FLOW_ACT_MANGLE_UNSPEC) {
-		NL_SET_ERR_MSG_MOD(extack, "legacy pedit isn't offloaded");
+	       	NL_SET_ERR_MSG_MOD(extack, "legacy pedit isn't offloaded");
 		goto out_err;
 	}
 
@@ -2580,15 +2849,17 @@ int alloc_tc_pedit_action(struct mlx5e_p
 			goto out_err;
 	}
 
-	err = offload_pedit_fields(hdrs, parse_attr, action_flags, extack);
+       err = offload_pedit_fields(hdrs, parse_attr, action_flags, extack);
 	if (err < 0)
 		goto out_dealloc_parsed_actions;
 
 	for (cmd = 0; cmd < __PEDIT_CMD_MAX; cmd++) {
 		cmd_masks = &hdrs[cmd].masks;
 		if (memcmp(cmd_masks, &zero_masks, sizeof(zero_masks))) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "attempt to offload an unsupported field");
+#endif
 			netdev_warn(priv->netdev, "attempt to offload an unsupported field (cmd %d)\n", cmd);
 			print_hex_dump(KERN_WARNING, "mask: ", DUMP_PREFIX_ADDRESS,
 				       16, 1, cmd_masks, sizeof(zero_masks), true);
@@ -2604,7 +2875,9 @@ out_dealloc_parsed_actions:
 out_err:
 	return err;
 }
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX */
 
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 static bool csum_offload_supported(struct mlx5e_priv *priv,
 				   u32 action,
 				   u32 update_flags,
@@ -2615,16 +2888,20 @@ static bool csum_offload_supported(struc
 
 	/*  The HW recalcs checksums only if re-writing headers */
 	if (!(action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "TC csum action is only offloaded with pedit");
+#endif
 		netdev_warn(priv->netdev,
 			    "TC csum action is only offloaded with pedit\n");
 		return false;
 	}
 
 	if (update_flags & ~prot_flags) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "can't offload TC csum action for some header/s");
+#endif
 		netdev_warn(priv->netdev,
 			    "can't offload TC csum action for some header/s - flags %#x\n",
 			    update_flags);
@@ -2633,7 +2910,9 @@ static bool csum_offload_supported(struc
 
 	return true;
 }
+#endif
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 struct ip_ttl_word {
 	__u8	ttl;
 	__u8	protocol;
@@ -2714,8 +2993,10 @@ static bool modify_header_match_supporte
 	ip_proto = MLX5_GET(fte_match_set_lyr_2_4, headers_v, ip_protocol);
 	if (modify_ip_header && ip_proto != IPPROTO_TCP &&
 	    ip_proto != IPPROTO_UDP && ip_proto != IPPROTO_ICMP) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "can't offload re-write of non TCP/UDP");
+#endif
 		pr_info("can't offload re-write of ip proto %d\n", ip_proto);
 		return false;
 	}
@@ -2723,6 +3004,7 @@ static bool modify_header_match_supporte
 out_ok:
 	return true;
 }
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX */
 
 static bool actions_match_supported(struct mlx5e_priv *priv,
 				    struct flow_action *flow_action,
@@ -2744,11 +3026,10 @@ static bool actions_match_supported(stru
 	      (actions & MLX5_FLOW_CONTEXT_ACTION_DROP)))
 		return false;
 #endif
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (actions & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
-		return modify_header_match_supported(&parse_attr->spec,
-						     flow_action, actions,
-						     extack);
-
+		return modify_header_match_supported(&parse_attr->spec, flow_action, actions, extack);
+#endif
 	return true;
 }
 
@@ -2766,11 +3047,13 @@ static bool same_hw_devs(struct mlx5e_pr
 	return (fsystem_guid == psystem_guid);
 }
 
+#if defined(HAVE_IS_TCF_VLAN) && defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX)
 static int add_vlan_rewrite_action(struct mlx5e_priv *priv, int namespace,
 				   const struct flow_action_entry *act,
 				   struct mlx5e_tc_flow_parse_attr *parse_attr,
 				   struct pedit_headers_action *hdrs,
-				   u32 *action, struct netlink_ext_ack *extack)
+				   u32 *action,
+				   struct netlink_ext_ack *extack)
 {
 	u16 mask16 = VLAN_VID_MASK;
 	u16 val16 = act->vlan.vid & VLAN_VID_MASK;
@@ -2790,16 +3073,15 @@ static int add_vlan_rewrite_action(struc
 
 	if (!(MLX5_GET(fte_match_set_lyr_2_4, headers_c, cvlan_tag) &&
 	      MLX5_GET(fte_match_set_lyr_2_4, headers_v, cvlan_tag))) {
-		NL_SET_ERR_MSG_MOD(extack,
-				   "VLAN rewrite action must have VLAN protocol match");
+	       	NL_SET_ERR_MSG_MOD(extack,
+       			   "VLAN rewrite action must have VLAN protocol match");
 		return -EOPNOTSUPP;
 	}
 
 	match_prio_mask = MLX5_GET(fte_match_set_lyr_2_4, headers_c, first_prio);
 	match_prio_val = MLX5_GET(fte_match_set_lyr_2_4, headers_v, first_prio);
 	if (act->vlan.prio != (match_prio_val & match_prio_mask)) {
-		NL_SET_ERR_MSG_MOD(extack,
-				   "Changing VLAN prio is not supported");
+		NL_SET_ERR_MSG_MOD(extack, "Changing VLAN prio is not supported");
 		return -EOPNOTSUPP;
 	}
 
@@ -2833,6 +3115,7 @@ add_vlan_prio_tag_rewrite_action(struct
 				       &prio_tag_act, parse_attr, hdrs, action,
 				       extack);
 }
+#endif /* defined(HAVE_IS_TCF_VLAN) && defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX) */
 
 static int parse_tc_nic_actions(struct mlx5e_priv *priv,
 				struct flow_action *flow_action,
@@ -2844,7 +3127,10 @@ static int parse_tc_nic_actions(struct m
 	struct pedit_headers_action hdrs[2] = {};
 	const struct flow_action_entry *act;
 	u32 action = 0;
-	int err, i;
+	int i;
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
+	int err;
+#endif
 
 	if (!flow_action_has_entries(flow_action))
 		return -EINVAL;
@@ -2859,6 +3145,7 @@ static int parse_tc_nic_actions(struct m
 					       flow_table_properties_nic_receive.flow_counter))
 				action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			break;
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 		case FLOW_ACTION_MANGLE:
 		case FLOW_ACTION_ADD:
 			err = parse_tc_pedit_action(priv, act, MLX5_FLOW_NAMESPACE_KERNEL,
@@ -2878,6 +3165,8 @@ static int parse_tc_nic_actions(struct m
 				return err;
 
 			break;
+#endif
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 		case FLOW_ACTION_CSUM:
 			if (csum_offload_supported(priv, action,
 						   act->csum_flags,
@@ -2885,6 +3174,7 @@ static int parse_tc_nic_actions(struct m
 				break;
 
 			return -EOPNOTSUPP;
+#endif
 		case FLOW_ACTION_REDIRECT: {
 			struct net_device *peer_dev = act->dev;
 
@@ -2895,8 +3185,10 @@ static int parse_tc_nic_actions(struct m
 				action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 					  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			} else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "device is not on same HW, can't offload");
+#endif
 				netdev_warn(priv->netdev, "device %s not on same HW, can't offload\n",
 					    peer_dev->name);
 				return -EINVAL;
@@ -2922,6 +3214,7 @@ static int parse_tc_nic_actions(struct m
 		}
 	}
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (hdrs[TCA_PEDIT_KEY_EX_CMD_SET].pedits ||
 	    hdrs[TCA_PEDIT_KEY_EX_CMD_ADD].pedits) {
 		err = alloc_tc_pedit_action(priv, MLX5_FLOW_NAMESPACE_KERNEL,
@@ -2936,6 +3229,7 @@ static int parse_tc_nic_actions(struct m
 			kfree(parse_attr->mod_hdr_actions);
 		}
 	}
+#endif
 
 	attr->action = action;
 	if (!actions_match_supported(priv, flow_action, parse_attr, flow, extack))
@@ -2949,6 +3243,7 @@ struct encap_key {
 	struct mlx5e_tc_tunnel *tc_tunnel;
 };
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static inline int cmp_encap_info(struct encap_key *a,
 				 struct encap_key *b)
 {
@@ -2961,7 +3256,7 @@ static inline int hash_encap_info(struct
 	return jhash(key->ip_tun_key, sizeof(*key->ip_tun_key),
 		     key->tc_tunnel->tunnel_type);
 }
-
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static bool is_merged_eswitch_dev(struct mlx5e_priv *priv,
 				  struct net_device *peer_netdev)
@@ -2976,8 +3271,7 @@ static bool is_merged_eswitch_dev(struct
 		same_hw_devs(priv, peer_priv));
 }
 
-
-
+#ifdef HAVE_TCF_TUNNEL_INFO
 bool mlx5e_encap_take(struct mlx5e_encap_entry *e)
 {
 	return refcount_inc_not_zero(&e->refcnt);
@@ -3081,7 +3375,11 @@ static int mlx5e_attach_encap(struct mlx
 	init_completion(&e->hw_res_created);
 
 	e->tun_info = tun_info;
-	err = mlx5e_tc_tun_init_encap_attr(mirred_dev, priv, e, extack);
+	err = mlx5e_tc_tun_init_encap_attr(mirred_dev, priv, e
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+					   , extack
+#endif
+					   );
 	if (err) {
 		kfree(e);
 		e = NULL;
@@ -3129,7 +3427,9 @@ out_err:
 		mlx5e_encap_put(priv, e);
 	return err;
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
+#ifdef HAVE_IS_TCF_VLAN
 static int parse_tc_vlan_action(struct mlx5e_priv *priv,
 				const struct flow_action_entry *act,
 				struct mlx5_esw_flow_attr *attr,
@@ -3189,7 +3489,7 @@ static int add_vlan_push_action(struct m
 				struct mlx5_esw_flow_attr *attr,
 				struct net_device **out_dev,
 				u32 *action,
-                                struct mlx5e_tc_flow_parse_attr *parse_attr,
+				struct mlx5e_tc_flow_parse_attr *parse_attr,
 				struct netlink_ext_ack *extack)
 {
 	struct net_device *vlan_dev = *out_dev;
@@ -3237,6 +3537,7 @@ static int add_vlan_pop_action(struct ml
 
 	return err;
 }
+#endif /* HAVE_IS_TCF_VLAN */
 
 /* This must be called under rcu_read_lock() */
 static struct net_device *get_active_rep_dev_from_lag(struct net_device *lag_dev)
@@ -3286,11 +3587,15 @@ static int parse_tc_fdb_actions(struct m
 				struct netlink_ext_ack *extack)
 {
 	struct pedit_headers_action hdrs[2] = {};
+#if defined(PRIO_CHAIN_SUPPORT) || defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX)
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#endif
 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 	struct mlx5e_tc_flow_parse_attr *parse_attr = attr->parse_attr;
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	const struct ip_tunnel_info *info = NULL;
+#endif
 	int ifindexes[MLX5_MAX_FLOW_FWD_VPORTS];
 	const struct flow_action_entry *act;
 	int err, i, if_count = 0;
@@ -3312,12 +3617,15 @@ static int parse_tc_fdb_actions(struct m
 			action |= MLX5_FLOW_CONTEXT_ACTION_DROP |
 				  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			break;
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 		case FLOW_ACTION_MANGLE:
 		case FLOW_ACTION_ADD:
+#ifdef HAVE_MINIFLOW
 			if (action & MLX5_FLOW_CONTEXT_ACTION_CT) {
 				pr_err("CT action before HDR is not allowed");
 				return -EOPNOTSUPP;
 			}
+#endif
 			err = parse_tc_pedit_action(priv, act, MLX5_FLOW_NAMESPACE_FDB,
 						    parse_attr, hdrs, extack);
 			if (err)
@@ -3326,12 +3634,16 @@ static int parse_tc_fdb_actions(struct m
 			action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;
 			attr->split_count = attr->out_count;
 			break;
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX */
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 		case FLOW_ACTION_CSUM:
 			if (csum_offload_supported(priv, action,
-						   act->csum_flags, extack))
+						   act->csum_flags,
+						   extack))
 				break;
 
 			return -EOPNOTSUPP;
+#endif
 		case FLOW_ACTION_REDIRECT:
 		case FLOW_ACTION_MIRRED: {
 			struct net_device *out_dev, *rep_dev;
@@ -3347,8 +3659,10 @@ static int parse_tc_fdb_actions(struct m
 			}
 
 			if (attr->out_count >= MLX5_MAX_FLOW_FWD_VPORTS) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "can't support more output ports, can't offload forwarding");
+#endif
 				pr_err("can't support more than %d output ports, can't offload forwarding\n",
 				       attr->out_count);
 				return -EOPNOTSUPP;
@@ -3357,6 +3671,7 @@ static int parse_tc_fdb_actions(struct m
 			action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 				  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			if (encap) {
+#ifdef HAVE_TCF_TUNNEL_INFO
 				parse_attr->mirred_ifindex[attr->out_count] =
 					out_dev->ifindex;
 				memcpy(&parse_attr->tun_info2[attr->out_count], info,
@@ -3370,7 +3685,13 @@ static int parse_tc_fdb_actions(struct m
 				/* attr->dests[].rep is resolved when we
 				 * handle encap
 				 */
+#endif
+#ifdef HAVE_NETDEV_PORT_SAME_PARENT_ID
 			} else if (netdev_port_same_parent_id(priv->netdev, out_dev)) {
+#else
+			} else if (switchdev_port_same_parent_id(priv->netdev,
+							  out_dev)) {
+#endif
 				struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 				struct net_device *uplink_dev = mlx5_eswitch_uplink_get_proto_dev(esw, REP_ETH);
 				struct net_device *uplink_upper;
@@ -3397,14 +3718,18 @@ static int parse_tc_fdb_actions(struct m
 					if (!rep_dev) {
 						rcu_read_unlock();
 						return -ENODEV;
+#ifdef HAVE_NETDEV_PORT_SAME_PARENT_ID
 					} else if (!netdev_port_same_parent_id(rep_dev, uplink_dev)) {
+#else
+					} else if (!switchdev_port_same_parent_id(rep_dev, uplink_dev)) {
+#endif
 						rcu_read_unlock();
 						return -EINVAL;
 					}
 					out_dev = rep_dev;
 				}
 				rcu_read_unlock();
-
+#ifdef HAVE_IS_TCF_VLAN
 				if (is_vlan_dev(out_dev)) {
 					err = add_vlan_push_action(priv, attr,
 								   &out_dev,
@@ -3423,12 +3748,16 @@ static int parse_tc_fdb_actions(struct m
 					if (err)
 						return err;
 				}
-
+#endif /* HAVE_IS_TCF_VLAN */
 				if (!mlx5e_is_valid_eswitch_fwd_dev(priv, out_dev)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 					NL_SET_ERR_MSG_MOD(extack,
 							   "devices are not on same switch HW, can't offload forwarding");
-					pr_err("devices %s %s not on same switch HW, can't offload forwarding\n",
-					       priv->netdev->name, out_dev->name);
+#endif
+					pr_err_once("devices %s %s not on same switch HW, can't offload forwarding\n",
+						    priv->netdev->name, out_dev->name);
+					pr_debug("devices %s %s not on same switch HW, can't offload forwarding\n",
+						 priv->netdev->name, out_dev->name);
 					return -EOPNOTSUPP;
 				}
 #ifdef HAVE_MINIFLOW
@@ -3451,8 +3780,10 @@ static int parse_tc_fdb_actions(struct m
 				 */
 				return -EINVAL;
 			} else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "devices are not on same switch HW, can't offload forwarding");
+#endif
 				pr_err_once("devices %s %s not on same switch HW, can't offload forwarding\n",
 					    priv->netdev->name, out_dev->name);
 				pr_debug("devices %s %s not on same switch HW, can't offload forwarding\n",
@@ -3461,33 +3792,47 @@ static int parse_tc_fdb_actions(struct m
 			}
 			}
 			break;
+#ifdef HAVE_TCF_TUNNEL_INFO
 		case FLOW_ACTION_TUNNEL_ENCAP:
+#ifdef CONFIG_COMPAT_TCF_TUNNEL_KEY_MOD
+			info = act->tunnel;
+#else
 			info = act->tunnel;
+#endif
 			if (info)
 				encap = true;
 			else
 				return -EOPNOTSUPP;
 
 			break;
+#endif
+#ifdef HAVE_IS_TCF_VLAN
 		case FLOW_ACTION_VLAN_PUSH:
 		case FLOW_ACTION_VLAN_POP:
 			if (act->id == FLOW_ACTION_VLAN_PUSH &&
 			    (action & MLX5_FLOW_CONTEXT_ACTION_VLAN_POP)) {
 				/* Replace vlan pop+push with vlan modify */
+#if defined(HAVE_IS_TCF_VLAN) && defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX)
 				action &= ~MLX5_FLOW_CONTEXT_ACTION_VLAN_POP;
 				err = add_vlan_rewrite_action(priv,
 							      MLX5_FLOW_NAMESPACE_FDB,
 							      act, parse_attr, hdrs,
-							      &action, extack);
+							      &action,
+							      extack);
+#else
+				err = -EOPNOTSUPP;
+#endif
 			} else {
 				err = parse_tc_vlan_action(priv, act, attr, &action,
-							   parse_attr, extack);
+							   parse_attr,
+							   extack);
 			}
 			if (err)
 				return err;
 
 			attr->split_count = attr->out_count;
 			break;
+#if defined(HAVE_IS_TCF_VLAN) && defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX)
 		case FLOW_ACTION_VLAN_MANGLE:
 			err = add_vlan_rewrite_action(priv,
 						      MLX5_FLOW_NAMESPACE_FDB,
@@ -3498,14 +3843,19 @@ static int parse_tc_fdb_actions(struct m
 
 			attr->split_count = attr->out_count;
 			break;
+#endif
+#endif /* HAVE_IS_TCF_VLAN */
+#ifdef HAVE_TCF_TUNNEL_INFO
 		case FLOW_ACTION_TUNNEL_DECAP:
 			action |= MLX5_FLOW_CONTEXT_ACTION_DECAP;
 			break;
+#endif
 #ifdef HAVE_MINIFLOW
 		case FLOW_ACTION_CT:
 			action |= MLX5_FLOW_CONTEXT_ACTION_CT;
 			continue;
 #endif
+#ifdef PRIO_CHAIN_SUPPORT
 		case FLOW_ACTION_GOTO: {
 			u32 dest_chain = act->chain_index;
 #ifndef HAVE_MINIFLOW
@@ -3534,12 +3884,14 @@ static int parse_tc_fdb_actions(struct m
 			action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			continue;
 		}
+#endif /* PRIO_CHAIN_SUPPORT */
 		default:
 			NL_SET_ERR_MSG_MOD(extack, "The offload action is not supported");
 			return -EOPNOTSUPP;
 		}
 	}
 
+#if defined(HAVE_IS_TCF_VLAN) && defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX)
 	if (MLX5_CAP_GEN(esw->dev, prio_tag_required) &&
 	    action & MLX5_FLOW_CONTEXT_ACTION_VLAN_POP) {
 		/* For prio tag mode, replace vlan pop with rewrite vlan prio
@@ -3551,7 +3903,9 @@ static int parse_tc_fdb_actions(struct m
 		if (err)
 			return err;
 	}
+#endif /* HAVE_IS_TCF_VLAN && HAVE_TCF_PEDIT_TCFP_KEYS_EX */
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (hdrs[TCA_PEDIT_KEY_EX_CMD_SET].pedits ||
 	    hdrs[TCA_PEDIT_KEY_EX_CMD_ADD].pedits) {
 		err = alloc_tc_pedit_action(priv, MLX5_FLOW_NAMESPACE_FDB,
@@ -3570,6 +3924,7 @@ static int parse_tc_fdb_actions(struct m
 				attr->split_count = 0;
 		}
 	}
+#endif
 
 #ifdef HAVE_MINIFLOW
 	if ((action & MLX5_FLOW_CONTEXT_ACTION_CT) &&
@@ -3587,8 +3942,9 @@ static int parse_tc_fdb_actions(struct m
 #endif
 
 	attr->action = action;
-	if (!actions_match_supported(priv, flow_action, parse_attr, flow, extack))
+	if (!actions_match_supported(priv, flow_action, parse_attr, flow, extack)) {
 		return -EOPNOTSUPP;
+	}
 
 	if (attr->dest_chain) {
 		if (attr->action & MLX5_FLOW_CONTEXT_ACTION_FWD_DEST) {
@@ -3599,8 +3955,10 @@ static int parse_tc_fdb_actions(struct m
 	}
 
 	if (attr->split_count > 0 && !mlx5_esw_has_fwd_fdb(priv->mdev)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "current firmware doesn't support split rule for port mirroring");
+#endif
 		netdev_warn_once(priv->netdev, "current firmware doesn't support split rule for port mirroring\n");
 		return -EOPNOTSUPP;
 	}
@@ -3612,6 +3970,7 @@ static void get_flags(int flags, unsigne
 {
 	unsigned long __flow_flags = 0;
 
+	/* relevant for the new ndo */
 	if (flags & MLX5_TC_FLAG(INGRESS))
 		__flow_flags |= BIT(MLX5E_TC_FLOW_FLAG_INGRESS);
 	if (flags & MLX5_TC_FLAG(EGRESS))
@@ -3632,13 +3991,29 @@ static const struct rhashtable_params tc
 	.automatic_shrinking = true,
 };
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+static void get_new_flags(struct mlx5e_priv *priv, unsigned long *flags)
+{
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+
+	if (mlx5e_eswitch_rep(priv->netdev) &&
+	    MLX5_VPORT_MANAGER(priv->mdev) && esw->mode == MLX5_ESWITCH_OFFLOADS)
+		*flags |= MLX5_TC_FLAG(ESW_OFFLOAD);
+}
+#endif
+
 static struct rhashtable *get_tc_ht(struct mlx5e_priv *priv,
 				    unsigned long flags)
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 	struct mlx5e_rep_priv *uplink_rpriv;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	if ((flags & MLX5_TC_FLAG(ESW_OFFLOAD)) ||
+	    mlx5e_eswitch_rep(priv->netdev)) {
+#else
 	if (flags & MLX5_TC_FLAG(ESW_OFFLOAD)) {
+#endif
 		uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
 		return &uplink_rpriv->uplink_priv.tc_ht;
 	} else /* NIC offload */
@@ -3648,22 +4023,32 @@ static struct rhashtable *get_tc_ht(stru
 static bool is_peer_flow_needed(struct mlx5e_tc_flow *flow)
 {
 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	bool is_rep_ingress = attr->in_rep->vport != MLX5_VPORT_UPLINK &&
 		flow_flag_test(flow, INGRESS);
 	bool act_is_encap = !!(attr->action &
 			       MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT);
+#endif
 	bool esw_paired = mlx5_devcom_is_paired(attr->in_mdev->priv.devcom,
 						MLX5_DEVCOM_ESW_OFFLOADS);
 
 	if (!esw_paired)
 		return false;
 
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	if ((mlx5_lag_is_sriov(attr->in_mdev) ||
 	     mlx5_lag_is_multipath(attr->in_mdev)) &&
-	    (is_rep_ingress || act_is_encap))
+	    (is_rep_ingress || act_is_encap
+#ifdef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+	     || (flow->flags & MLX5_TC_FLAG(EGRESS))
+#endif
+))
 		return true;
 
 	return false;
+#else
+	return (mlx5_lag_is_sriov(attr->in_mdev) ||  mlx5_lag_is_multipath(attr->in_mdev));
+#endif
 }
 
 void *mlx5e_lookup_tc_ht(struct mlx5e_priv *priv,
@@ -3695,9 +4080,13 @@ mlx5e_alloc_flow(struct mlx5e_priv *priv
 	flow->flags = flow_flags;
 	flow->cookie = cookie;
 	flow->priv = priv;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++)
 		INIT_LIST_HEAD(&flow->encaps[out_index].list);
+#endif
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	INIT_LIST_HEAD(&flow->mod_hdr);
+#endif
 	INIT_LIST_HEAD(&flow->hairpin);
 	refcount_set(&flow->refcnt, 1);
 	init_completion(&flow->init_done);
@@ -3726,8 +4115,12 @@ mlx5e_flow_esw_attr_init(struct mlx5_esw
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 
 	esw_attr->parse_attr = parse_attr;
+#ifdef PRIO_CHAIN_SUPPORT
 	esw_attr->chain = f->common.chain_index;
 	esw_attr->prio = TC_H_MAJ(f->common.prio) >> 16;
+#else
+	esw_attr->prio = 1;
+#endif
 
 	esw_attr->in_rep = in_rep;
 	esw_attr->in_mdev = in_mdev;
@@ -3752,6 +4145,53 @@ static bool is_flow_simple(struct mlx5e_
 	return true;
 }
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+static void build_rule_match(struct tc_cls_flower_offload *f,
+			     struct flow_match *match)
+{
+	match->dissector = f->dissector;
+	match->mask = f->mask;
+	match->key = f->key;
+}
+
+static int build_rule_action(struct tc_cls_flower_offload *f,
+			     struct flow_rule *rule)
+{
+	return tc_setup_flow_action(&rule->action, f->exts);
+}
+
+struct flow_rule *alloc_flow_rule(struct tc_cls_flower_offload *f)
+{
+	struct flow_rule *ret;
+	int num_ent;
+	int err;
+
+	num_ent = tcf_exts_num_actions(f->exts);
+	ret = kzalloc(sizeof(*ret) + num_ent * sizeof(ret->action.entries[0]),
+		      GFP_KERNEL);
+	if (!ret)
+		return ERR_PTR(-ENOMEM);
+
+	ret->action.num_entries = num_ent;
+	err = build_rule_action(f, ret);
+	if (err)
+		goto out;
+
+	build_rule_match(f, &ret->match);
+
+	return ret;
+
+out:
+	kfree(ret);
+	return ERR_PTR(err);
+}
+
+static void free_flow_rule(struct flow_rule *rule)
+{
+	kfree(rule);
+}
+#endif
+
 static struct mlx5e_tc_flow *
 __mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
 		     struct tc_cls_flower_offload *f,
@@ -3760,12 +4200,24 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 		     struct mlx5_eswitch_rep *in_rep,
 		     struct mlx5_core_dev *in_mdev)
 {
-	struct flow_rule *rule = tc_cls_flower_offload_flow_rule(f);
+	struct flow_rule *rule;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#else
+	struct netlink_ext_ack *extack = NULL;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
 	int attr_size, err;
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = alloc_flow_rule(f);
+	if (IS_ERR(rule))
+		return ERR_PTR(PTR_ERR(rule));
+#else
+	rule = tc_cls_flower_offload_flow_rule(f);
+#endif
+
 	flow_flags |= BIT(MLX5E_TC_FLOW_FLAG_ESWITCH) | BIT(MLX5E_TC_FLOW_FLAG_SIMPLE);
 	attr_size  = sizeof(struct mlx5_esw_flow_attr);
 	err = mlx5e_alloc_flow(priv, attr_size, f->cookie, flow_flags, GFP_KERNEL,
@@ -3777,9 +4229,12 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 	mlx5e_flow_esw_attr_init(flow->esw_attr,
 				 priv, parse_attr,
 				 f, in_rep, in_mdev);
-
 	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
-			       f, filter_dev);
+			       f, filter_dev
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+			       , rule
+#endif
+					    );
 	if (err)
 		goto err_free;
 
@@ -3800,11 +4255,20 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 		flow_flag_clear(flow, SIMPLE);
 	}
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
 	return flow;
 
 err_free:
 	mlx5e_flow_put(priv, flow);
 out:
+#if !defined(HAVE_IS_TCF_TUNNEL) && defined(HAVE_TCF_TUNNEL_INFO)
+#endif
+
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
 	return ERR_PTR(err);
 }
 
@@ -3903,15 +4367,29 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 		   struct net_device *filter_dev,
 		   struct mlx5e_tc_flow **__flow)
 {
-	struct flow_rule *rule = tc_cls_flower_offload_flow_rule(f);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#else
+	struct netlink_ext_ack *extack = NULL;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
+	struct flow_rule *rule;
 	int attr_size, err;
 
-	/* multi-chain not supported for NIC rules */
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && defined(PRIO_CHAIN_SUPPORT)
+       /* multi-chain not supported for NIC rules */
 	if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
 		return -EOPNOTSUPP;
+#endif /* HAVE_TC_CLS_OFFLOAD_EXTACK && PRIO_CHAIN_SUPPORT */
+
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = alloc_flow_rule(f);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+#else
+	rule = tc_cls_flower_offload_flow_rule(f);
+#endif
 
 	flow_flags |= BIT(MLX5E_TC_FLOW_FLAG_NIC) | BIT(MLX5E_TC_FLOW_FLAG_SIMPLE);
 	attr_size  = sizeof(struct mlx5_nic_flow_attr);
@@ -3922,7 +4400,12 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 
 	parse_attr->filter_dev = filter_dev;
 	err = parse_cls_flower(flow->priv, flow, &parse_attr->spec,
-			       f, filter_dev);
+			       f, filter_dev
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+			       , rule
+#endif
+
+					    );
 	if (err)
 		goto err_free;
 
@@ -3936,6 +4419,9 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 
 	flow_flag_set(flow, OFFLOADED);
 	kvfree(parse_attr);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
 	*__flow = flow;
 
 	return 0;
@@ -3944,6 +4430,9 @@ err_free:
 	mlx5e_flow_put(priv, flow);
 	kvfree(parse_attr);
 out:
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
 	return err;
 }
 
@@ -3959,9 +4448,10 @@ mlx5e_tc_add_flow(struct mlx5e_priv *pri
 	int err;
 
 	get_flags(flags, &flow_flags);
-
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON)
 	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
 		return -EOPNOTSUPP;
+#endif
 
 	if (esw && esw->mode == MLX5_ESWITCH_OFFLOADS)
 		err = mlx5e_add_fdb_flow(priv, f, flow_flags,
@@ -3985,12 +4475,18 @@ static bool is_flow_rule_duplicate_allow
 int mlx5e_configure_flower(struct net_device *dev, struct mlx5e_priv *priv,
 			   struct tc_cls_flower_offload *f, unsigned long flags)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 	struct mlx5e_tc_flow *flow;
 	int err = 0;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	get_new_flags(priv, &flags);
+#endif
+
 	flow = rhashtable_lookup_fast(tc_ht, &f->cookie, tc_ht_params);
 	if (flow) {
 		/* Same flow rule offloaded to non-uplink representor sharing
@@ -4000,8 +4496,10 @@ int mlx5e_configure_flower(struct net_de
 		    flow->added_dev != dev)
 			goto out;
 
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "flow cookie already exists, ignoring");
+#endif
 		netdev_warn_once(priv->netdev,
 				 "flow cookie %lx already exists, ignoring\n",
 				 f->cookie);
@@ -4033,14 +4531,21 @@ err_free:
 out:
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_configure_flower);
+#endif
 
 static bool same_flow_direction(struct mlx5e_tc_flow *flow, int flags)
 {
-	bool dir_ingress = !!(flags & MLX5_TC_FLAG(INGRESS));
-	bool dir_egress = !!(flags & MLX5_TC_FLAG(EGRESS));
+#if !(defined(HAVE_NDO_SETUP_TC_4_PARAMS) || defined(HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX))
+       bool dir_ingress = !!(flags & MLX5_TC_FLAG(INGRESS));
+       bool dir_egress = !!(flags & MLX5_TC_FLAG(EGRESS));
 
-	return flow_flag_test(flow, INGRESS) == dir_ingress &&
-		flow_flag_test(flow, EGRESS) == dir_egress;
+       return flow_flag_test(flow, INGRESS) == dir_ingress &&
+       	flow_flag_test(flow, EGRESS) == dir_egress;
+#else
+	return true;
+#endif
 }
 
 static void mlx5e_flow_defered_put(struct rcu_head *head)
@@ -4089,7 +4594,11 @@ errout:
 	rcu_read_unlock();
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_delete_flower);
+#endif
 
+#ifdef HAVE_TC_CLSFLOWER_STATS
 int mlx5e_stats_flower(struct net_device *dev, struct mlx5e_priv *priv,
 		       struct tc_cls_flower_offload *f, unsigned long flags)
 {
@@ -4098,11 +4607,19 @@ int mlx5e_stats_flower(struct net_device
 	struct mlx5_eswitch *peer_esw;
 	struct mlx5e_tc_flow *flow;
 	struct mlx5_fc *counter;
+#ifndef HAVE_TCF_EXTS_STATS_UPDATE
+	struct tc_action *a;
+	LIST_HEAD(actions);
+#endif
 	u64 lastuse = 0;
 	u64 packets = 0;
 	u64 bytes = 0;
 	int err = 0;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	get_new_flags(priv, &flags);
+#endif
+
 	rcu_read_lock();
 	flow = mlx5e_flow_get(rhashtable_lookup(tc_ht, &f->cookie,
 						tc_ht_params));
@@ -4151,12 +4668,47 @@ int mlx5e_stats_flower(struct net_device
 no_peer_counter:
 	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
 out:
-	flow_stats_update(&f->stats, bytes, packets, lastuse);
+#ifdef HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD
+	f->stats.pkts += packets;
+	f->stats.bytes += bytes;
+	f->stats.lastused = max_t(u64, f->stats.lastused, lastuse);
+#elif defined(HAVE_TCF_EXTS_STATS_UPDATE)
+	tcf_exts_stats_update(f->exts, bytes, packets, lastuse);
+#else
+	preempt_disable();
+
+#ifdef HAVE_TCF_EXTS_TO_LIST
+	tcf_exts_to_list(f->exts, &actions);
+	list_for_each_entry(a, &actions, list)
+#else
+	tc_for_each_action(a, f->exts)
+#endif
+#ifdef HAVE_TCF_ACTION_STATS_UPDATE
+		tcf_action_stats_update(a, bytes, packets, lastuse);
+#else
+	{
+		struct tcf_act_hdr *h = a->priv;
+
+		spin_lock(&h->tcf_lock);
+		h->tcf_tm.lastuse = max_t(u64, h->tcf_tm.lastuse, lastuse);
+		h->tcf_bstats.bytes += bytes;
+		h->tcf_bstats.packets += packets;
+		spin_unlock(&h->tcf_lock);
+	}
+#endif
+	preempt_enable();
+#endif /* HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD */
+
 errout:
 	mlx5e_flow_put(priv, flow);
 	return err;
 }
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_stats_flower);
+#endif
+#endif /* HAVE_TC_CLSFLOWER_STATS */
+
 static void mlx5e_tc_hairpin_update_dead_peer(struct mlx5e_priv *priv,
 					      struct mlx5e_priv *peer_priv)
 {
@@ -4221,15 +4773,19 @@ static int mlx5e_tc_netdev_event(struct
 
 	return NOTIFY_DONE;
 }
+#endif /* HAVE_TC_FLOWER_OFFLOAD */
 
 int mlx5e_tc_nic_init(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_table *tc = &priv->fs.tc;
 	int err;
 
 	mutex_init(&tc->t_lock);
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	mutex_init(&tc->mod_hdr.lock);
 	hash_init(tc->mod_hdr.hlist);
+#endif
 	mutex_init(&tc->hairpin_tbl_lock);
 	hash_init(tc->hairpin_tbl);
 
@@ -4244,8 +4800,12 @@ int mlx5e_tc_nic_init(struct mlx5e_priv
 	}
 
 	return err;
+#else
+	return 0;
+#endif
 }
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 static void _mlx5e_tc_del_flow(void *ptr, void *arg)
 {
 	struct mlx5e_tc_flow *flow = ptr;
@@ -4254,15 +4814,19 @@ static void _mlx5e_tc_del_flow(void *ptr
 	mlx5e_tc_del_flow(priv, flow);
 	kfree(flow);
 }
+#endif
 
 void mlx5e_tc_nic_cleanup(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_table *tc = &priv->fs.tc;
 
 	if (tc->netdevice_nb.notifier_call)
 		unregister_netdevice_notifier(&tc->netdevice_nb);
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	mutex_destroy(&tc->mod_hdr.lock);
+#endif
 	mutex_destroy(&tc->hairpin_tbl_lock);
 
 	rhashtable_destroy(&tc->ht);
@@ -4272,10 +4836,12 @@ void mlx5e_tc_nic_cleanup(struct mlx5e_p
 		tc->t = NULL;
 	}
 	mutex_destroy(&tc->t_lock);
+#endif
 }
 
 int mlx5e_tc_esw_init(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct rhashtable *tc_ht = get_tc_ht(priv, MLX5_TC_FLAG(ESW_OFFLOAD));
 	int err;
 
@@ -4292,33 +4858,44 @@ int mlx5e_tc_esw_init(struct mlx5e_priv
 err_tc_ht:
 	miniflow_cache_destroy(priv);
 	return err;
+#else
+	return 0;
+#endif
 }
 
 void mlx5e_tc_esw_cleanup(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct rhashtable *tc_ht = get_tc_ht(priv, MLX5_TC_FLAG(ESW_OFFLOAD));
 
 	rhashtable_free_and_destroy(tc_ht, _mlx5e_tc_del_flow, NULL);
 	miniflow_cache_destroy(priv);
+#endif
 }
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 int mlx5e_tc_num_filters(struct mlx5e_priv *priv, unsigned long flags)
 {
 	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 
 	return atomic_read(&tc_ht->nelems);
 }
+#endif
 
 void mlx5e_tc_clean_fdb_peer_flows(struct mlx5_eswitch *esw)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_flow *flow, *tmp;
 
 	list_for_each_entry_safe(flow, tmp, &esw->offloads.peer_flows, peer)
 		__mlx5e_tc_del_fdb_peer_flow(flow);
+#endif
 }
 
 void mlx5e_tc_reoffload_flows_work(struct work_struct *work)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct mlx5_rep_uplink_priv *rpriv =
 		container_of(work, struct mlx5_rep_uplink_priv,
 			     reoffload_flows_work);
@@ -4330,4 +4907,6 @@ void mlx5e_tc_reoffload_flows_work(struc
 			unready_flow_del(flow);
 	}
 	mutex_unlock(&rpriv->unready_flows_lock);
+#endif
+#endif
 }
